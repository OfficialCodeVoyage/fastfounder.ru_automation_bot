#!/usr/bin/env python3
"""
FastFounder Daily Link-Push Bot - Authenticated Version

Enhanced version with authentication to access full article content.
Provides comprehensive analysis based on complete articles (11,000+ characters).
"""

import json
import os
import urllib.request
import urllib.parse
import urllib.error
import http.cookiejar
import xml.etree.ElementTree as ET
from html import unescape
import re
import logging
import sys
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)


class FastFounderAuthenticatedScraper:
    """Authenticated scraper for FastFounder articles."""
    
    def __init__(self):
        # Set up cookie jar for session management
        self.cookie_jar = http.cookiejar.CookieJar()
        self.opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(self.cookie_jar))
        
        # Add headers to mimic a real browser
        self.opener.addheaders = [
            ('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'),
            ('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'),
            ('Accept-Language', 'ru-RU,ru;q=0.9,en;q=0.8'),
            ('Connection', 'keep-alive'),
            ('Upgrade-Insecure-Requests', '1'),
        ]
        
        # Install the opener
        urllib.request.install_opener(self.opener)
        
        self.logged_in = False
        
    def login(self, email, password):
        """Login to FastFounder."""
        logger.info("üîê Attempting to login to FastFounder...")
        
        try:
            # First, get the login page to extract any CSRF tokens or form data
            login_page_url = "https://fastfounder.ru/wp-login.php"
            
            req = urllib.request.Request(login_page_url)
            req.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')
            
            with urllib.request.urlopen(req) as response:
                login_page_data = response.read()
                try:
                    login_page_html = login_page_data.decode('utf-8')
                except UnicodeDecodeError:
                    login_page_html = login_page_data.decode('utf-8', errors='ignore')
            
            # Prepare login data - using the correct WordPress form fields
            login_data = {
                'log': email,  # Username or email field
                'pwd': password,  # Password field
                'wp-submit': '–í–æ–π—Ç–∏',  # Submit button
                'redirect_to': 'https://fastfounder.ru/',  # Redirect after login
                'testcookie': '1',
                'rememberme': 'forever'  # Remember me checkbox
            }
            
            # Encode the data
            encoded_data = urllib.parse.urlencode(login_data).encode('utf-8')
            
            # Create the request
            login_request = urllib.request.Request(
                "https://fastfounder.ru/wp-login.php",
                data=encoded_data,
                headers={
                    'Content-Type': 'application/x-www-form-urlencoded',
                    'Referer': 'https://fastfounder.ru/wp-login.php',
                    'Origin': 'https://fastfounder.ru'
                }
            )
            
            # Submit login
            with urllib.request.urlopen(login_request) as response:
                login_response_data = response.read()
                try:
                    login_response = login_response_data.decode('utf-8')
                except UnicodeDecodeError:
                    login_response = login_response_data.decode('utf-8', errors='ignore')
                response_url = response.geturl()
            
            logger.info(f"üìç Login response URL: {response_url}")
            
            # Check if login was successful
            if 'wp-login.php' not in response_url and 'loggedout' not in response_url:
                logger.info("‚úÖ Login successful!")
                self.logged_in = True
                return True
            else:
                logger.error("‚ùå Login failed - redirected back to login page")
                if 'loggedout=true' in response_url:
                    logger.info("üîç System indicates we were logged out")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Login error: {e}")
            return False
    
    def _test_authentication(self):
        """Test if we're properly authenticated."""
        try:
            test_url = "https://fastfounder.ru/"
            
            with urllib.request.urlopen(test_url) as response:
                test_content = response.read().decode('utf-8')
            
            # Look for indicators of being logged in
            logged_in_indicators = ['logout', '–≤—ã–π—Ç–∏', 'profile', '–ø—Ä–æ—Ñ–∏–ª—å', 'dashboard', 'account']
            
            if any(indicator in test_content.lower() for indicator in logged_in_indicators):
                logger.info("‚úÖ Authentication confirmed!")
                self.logged_in = True
                return True
            else:
                logger.error("‚ùå Authentication failed")
                self.logged_in = False
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Authentication test error: {e}")
            return False
    
    def get_full_article_content(self, article_url):
        """Get full article content using authenticated session."""
        if not self.logged_in:
            logger.error("‚ùå Not logged in! Cannot access full content.")
            return None
        
        logger.info(f"üìñ Fetching full authenticated article content...")
        
        try:
            with urllib.request.urlopen(article_url) as response:
                html_content = response.read().decode('utf-8')
            
            # Extract clean content
            clean_content = self._extract_clean_content(html_content)
            
            logger.info(f"‚úÖ Extracted {len(clean_content)} characters of authenticated content")
            return clean_content
            
        except Exception as e:
            logger.error(f"‚ùå Error fetching authenticated article: {e}")
            return None
    
    def _extract_clean_content(self, html_content):
        """Extract clean article content from HTML."""
        
        # Remove script tags and their content
        html_content = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
        
        # Remove style tags and their content
        html_content = re.sub(r'<style[^>]*>.*?</style>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
        
        # Remove comments
        html_content = re.sub(r'<!--.*?-->', '', html_content, flags=re.DOTALL)
        
        # Convert to text
        text_content = re.sub(r'<[^>]+>', ' ', html_content)
        text_content = re.sub(r'\s+', ' ', text_content).strip()
        
        # Look for the article title and start from there
        title_patterns = [
            r'–ü—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± –æ—Ç–∫—É—Å–∏—Ç—å –∫—É—Å–æ—á–µ–∫ –æ–≥—Ä–æ–º–Ω—ã—Ö –±—é–¥–∂–µ—Ç–æ–≤',
            r'–°—Ç–∞—Ä—Ç–∞–ø –ø–æ–ª—É—á–∏–ª \$80M',
            r'–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ \d{2}\.\d{2}\.\d{4}',
        ]
        
        article_start = 0
        for pattern in title_patterns:
            match = re.search(pattern, text_content)
            if match:
                article_start = match.start()
                logger.info(f"üìç Found article start at '{pattern}' (position {article_start})")
                break
        
        # If no title found, look for content markers
        if article_start == 0:
            content_markers = [
                '–°—É—Ç—å –ø—Ä–æ–µ–∫—Ç–∞',
                'VuMedi ‚Äî —ç—Ç–æ –º–µ—Å—Ç–æ',
                '#–∑–¥–æ—Ä–æ–≤—å–µ ‚Ä¢ #–º–∞—Ä–∫–µ—Ç–∏–Ω–≥',
                '–£–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ',
            ]
            
            for marker in content_markers:
                pos = text_content.find(marker)
                if pos != -1:
                    article_start = pos
                    logger.info(f"üìç Found content start at '{marker}' (position {pos})")
                    break
        
        # Look for the end of the article (before comments/footer)
        end_markers = [
            '–î–æ–±–∞–≤–∏—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π',
            '–í—ã –≤–æ—à–ª–∏ –∫–∞–∫',
            '¬© –ê—Ä–∫–∞–¥–∏–π –ú–æ—Ä–µ–π–Ω–∏—Å',
            'ff@fastfounder.ru',
            '–ü—É–±–ª–∏—á–Ω–∞—è –æ—Ñ–µ—Ä—Ç–∞'
        ]
        
        article_end = len(text_content)
        for marker in end_markers:
            pos = text_content.find(marker, article_start)
            if pos != -1 and pos < article_end:
                article_end = pos
                logger.info(f"üìç Found article end at '{marker}' (position {pos})")
                break
        
        # Extract the article content
        if article_start < article_end:
            clean_text = text_content[article_start:article_end].strip()
            logger.info(f"üìÑ Extracted article: {len(clean_text)} characters (from {article_start} to {article_end})")
        else:
            # Fallback: skip first 500 characters
            clean_text = text_content[500:].strip() if len(text_content) > 500 else text_content
            logger.info(f"üìÑ Fallback extraction: {len(clean_text)} characters")
        
        return clean_text


def get_rss_feed():
    """Get RSS feed items using only standard library."""
    logger.info("üì° Fetching RSS feed...")
    
    try:
        # Create request without gzip encoding
        req = urllib.request.Request("https://fastfounder.ru/feed/")
        req.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')
        
        # Fetch the RSS feed
        with urllib.request.urlopen(req) as response:
            rss_data = response.read()
            
            # Try to decode with different encodings
            try:
                rss_text = rss_data.decode('utf-8')
            except UnicodeDecodeError:
                try:
                    rss_text = rss_data.decode('cp1251')
                except UnicodeDecodeError:
                    rss_text = rss_data.decode('utf-8', errors='ignore')
        
        # Parse XML
        root = ET.fromstring(rss_text)
        
        # Find all items
        items = []
        for item in root.findall('.//item'):
            title_elem = item.find('title')
            link_elem = item.find('link')
            desc_elem = item.find('description')
            
            if title_elem is not None and link_elem is not None:
                title = unescape(title_elem.text or "")
                link = link_elem.text or ""
                description = unescape(desc_elem.text or "") if desc_elem is not None else ""
                
                # Clean up description (remove HTML tags)
                description = re.sub(r'<[^>]+>', '', description)
                description = description.strip()
                
                items.append({
                    'title': title,
                    'link': link,
                    'description': description
                })
        
        logger.info(f"‚úÖ Found {len(items)} articles in RSS feed")
        return items[:1]  # Return only the latest article for daily digest
        
    except Exception as e:
        logger.error(f"‚ùå Error fetching RSS feed: {e}")
        return []


def scrape_article_content_authenticated(article, scraper):
    """Scrape full article content using authentication."""
    logger.info(f"üîç Scraping authenticated article content: {article['title']}")
    
    # Try authenticated scraping first
    if scraper and scraper.logged_in:
        full_content = scraper.get_full_article_content(article['link'])
        if full_content and len(full_content) > 1000:
            return {
                'title': article['title'],
                'content': full_content,
                'url': article['link'],
                'description': article['description'],
                'content_source': 'authenticated'
            }
    
    # Direct content extraction - get everything before paywall
    logger.warning("‚ö†Ô∏è Extracting all content before paywall")
    try:
        req = urllib.request.Request(article['link'])
        req.add_header('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')
        
        with urllib.request.urlopen(req) as response:
            html_content = response.read().decode('utf-8', errors='ignore')
        
        # Extract ALL text content (minimal filtering)
        text_content = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
        text_content = re.sub(r'<style[^>]*>.*?</style>', '', text_content, flags=re.DOTALL | re.IGNORECASE)
        text_content = re.sub(r'<[^>]+>', ' ', text_content)
        text_content = re.sub(r'\s+', ' ', text_content).strip()
        
        # Find paywall and take EVERYTHING before it
        paywall_indicators = [
            '—á–∏—Ç–∞—Ç—å –¥–∞–ª—å—à–µ ‚û§ –ø–æ–¥–ø–∏—à–∏—Å—å',
            '–ø–æ–¥–ø–∏—à–∏—Å—å , —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç',
            '–ø–æ–¥–ø–∏—à–∏—Å—å, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç',
            '—á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –æ–±–∑–æ—Ä–∞',
        ]
        
        # Find the earliest paywall position
        paywall_pos = len(text_content)
        
        for indicator in paywall_indicators:
            pos = text_content.lower().find(indicator.lower())
            if pos != -1 and pos < paywall_pos:
                paywall_pos = pos
                logger.info(f"üîí Paywall found at position {pos}: '{indicator}'")
        
        # Take all content before paywall
        if paywall_pos < len(text_content):
            content = text_content[:paywall_pos].strip()
            logger.info(f"‚úÖ Extracted {len(content)} characters before paywall")
        else:
            content = text_content
            logger.info(f"‚úÖ No paywall found, using all {len(content)} characters")
        
        # Minimal cleaning - just remove obvious junk at the start
        content = minimal_clean(content)
        
        return {
            'title': article['title'],
            'content': content,
            'url': article['link'],
            'description': article['description'],
            'content_source': 'full_extraction'
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error scraping article: {e}")
        # Final fallback to RSS description
        return {
            'title': article['title'],
            'content': article['description'],
            'url': article['link'],
            'description': article['description'],
            'content_source': 'rss_only'
        }


def minimal_clean(content):
    """Minimal cleaning - just remove obvious technical junk."""
    # Skip to actual article content
    article_markers = [
        '1. –£–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ',
        '–£–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ',
        '—Å–∞–º—ã–π –±—ã—Å—Ç—Ä–æ—Ä–∞—Å—Ç—É—â–∏–π —Å–µ–≥–º–µ–Ω—Ç',
    ]
    
    for marker in article_markers:
        pos = content.find(marker)
        if pos != -1:
            logger.info(f"üìç Found article start at '{marker}' (position {pos})")
            # Include everything from this marker onwards
            return content[pos:].strip()
    
    # Fallback: look for date pattern and start from there
    date_pattern = re.search(r'–û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ \d{2}\.\d{2}\.\d{4}', content)
    if date_pattern:
        start_pos = date_pattern.end()
        logger.info(f"üìç Found date pattern, starting from position {start_pos}")
        return content[start_pos:].strip()
    
    # Last resort: skip first 400 chars of navigation/header
    if len(content) > 500:
        logger.info("üìç Using fallback: skipping first 400 characters")
        return content[400:].strip()
    
    return content


def generate_enhanced_analysis(article):
    """Generate enhanced comprehensive analysis using GPT-4o mini."""
    logger.info(f"ü§ñ Generating enhanced analysis for: {article['title']}")
    
    openai_api_key = os.environ.get('OPENAI_API_KEY')
    if not openai_api_key:
        logger.warning("‚ùå OpenAI API key not found, using fallback analysis")
        return create_fallback_analysis(article)
    
    # Adjust content length based on source
    content_for_analysis = article['content']
    if article.get('content_source') == 'authenticated':
        # Use more content for authenticated articles (up to 8000 chars)
        content_for_analysis = content_for_analysis[:8000]
        logger.info(f"üìä Using {len(content_for_analysis)} characters of authenticated content for analysis")
    elif article.get('content_source') == 'full_extraction':
        # Use substantial content for full extraction (up to 8000 chars)
        content_for_analysis = content_for_analysis[:8000]
        logger.info(f"üìä Using {len(content_for_analysis)} characters of full extraction content for analysis")
    elif article.get('content_source') == 'improved_extraction':
        # Use substantial content for improved extraction (up to 6000 chars)
        content_for_analysis = content_for_analysis[:6000]
        logger.info(f"üìä Using {len(content_for_analysis)} characters of improved extraction content for analysis")
    else:
        # Use standard amount for fallback content
        content_for_analysis = content_for_analysis[:3500]
        logger.info(f"üìä Using {len(content_for_analysis)} characters of fallback content for analysis")
    
    # Prepare the enhanced prompt
    prompt = f"""
    –ü—Ä–æ–≤–µ–¥–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç–∞—Ç—å–∏ –∏ —Å–æ–∑–¥–∞–π –ø–æ–ª–Ω—É—é —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É.
    
    –ó–∞–≥–æ–ª–æ–≤–æ–∫: {article['title']}
    
    –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:
    {content_for_analysis}
    
    –í–ê–ñ–ù–û: –ö–æ–Ω—Ç–µ–Ω—Ç –ø–æ–ª—É—á–µ–Ω —á–µ—Ä–µ–∑ {'–∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é (–ø–æ–ª–Ω–∞—è —Å—Ç–∞—Ç—å—è)' if article.get('content_source') == 'authenticated' else '–ø—É–±–ª–∏—á–Ω—ã–π –¥–æ—Å—Ç—É–ø (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–ø–æ–ª–Ω—ã–º)'}.
    
    –®–ö–ê–õ–´ –û–¶–ï–ù–ö–ò (1-10):
    - –ü—Ä–∞–∫—Ç–∏—á–Ω–æ—Å—Ç—å: –Ω–∞—Å–∫–æ–ª—å–∫–æ –ª–µ–≥–∫–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ
    - –ù–æ–≤–∏–∑–Ω–∞: —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∏–¥–µ–π
    - –ì–ª—É–±–∏–Ω–∞: –¥–µ—Ç–∞–ª—å–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–±–æ—Ä–∞
    - –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å: —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
    
    –¶–ï–õ–ï–í–ê–Ø –ê–£–î–ò–¢–û–†–ò–Ø (–≤—ã–±–µ—Ä–∏ 1-2 –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö):
    - –ù–∞—á–∏–Ω–∞—é—â–∏–µ –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–∏
    - –û–ø—ã—Ç–Ω—ã–µ –±–∏–∑–Ω–µ—Å–º–µ–Ω—ã
    - –ú–∞—Ä–∫–µ—Ç–æ–ª–æ–≥–∏
    - IT-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã
    - –§—Ä–∏–ª–∞–Ω—Å–µ—Ä—ã
    - –ò–Ω–≤–µ—Å—Ç–æ—Ä—ã
    - –°—Ç—É–¥–µ–Ω—Ç—ã
    
    –í–†–ï–ú–Ø –ù–ê –ò–ó–£–ß–ï–ù–ò–ï:
    - –ë—ã—Å—Ç—Ä–æ (5-10 –º–∏–Ω)
    - –°—Ä–µ–¥–Ω–µ (15-30 –º–∏–Ω)
    - –î–æ–ª–≥–æ (1+ —á–∞—Å)
    
    –ö–ê–¢–ï–ì–û–†–ò–Ø:
    - –°—Ç—Ä–∞—Ç–µ–≥–∏—è
    - –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥
    - –ü—Ä–æ–¥–∞–∂–∏
    - –§–∏–Ω–∞–Ω—Å—ã
    - –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
    - –õ–∏—á–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
    - –ê–Ω–∞–ª–∏—Ç–∏–∫–∞
    
    –£–†–û–í–ï–ù–¨ –°–õ–û–ñ–ù–û–°–¢–ò:
    - –ü—Ä–æ—Å—Ç–æ–π (–º–æ–∂–Ω–æ –≤–Ω–µ–¥—Ä–∏—Ç—å –∑–∞ –¥–µ–Ω—å)
    - –°—Ä–µ–¥–Ω–∏–π (—Ç—Ä–µ–±—É–µ—Ç –Ω–µ–¥–µ–ª–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è)
    - –°–ª–æ–∂–Ω—ã–π (–Ω—É–∂–Ω–∞ —Å–µ—Ä—å–µ–∑–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞)
    
    ROI –ü–û–¢–ï–ù–¶–ò–ê–õ:
    - –í—ã—Å–æ–∫–∏–π (–±—ã—Å—Ç—Ä–∞—è –æ–∫—É–ø–∞–µ–º–æ—Å—Ç—å)
    - –°—Ä–µ–¥–Ω–∏–π (–¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –≤—ã–≥–æ–¥–∞)
    - –ù–∏–∑–∫–∏–π (–±–æ–ª—å—à–µ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è)
    
    –°–¢–ê–î–ò–Ø –ë–ò–ó–ù–ï–°–ê:
    - –°—Ç–∞—Ä—Ç–∞–ø (0-2 –≥–æ–¥–∞)
    - –†–æ—Å—Ç (2-5 –ª–µ—Ç)
    - –ó—Ä–µ–ª—ã–π –±–∏–∑–Ω–µ—Å (5+ –ª–µ—Ç)
    
    –í–†–ï–ú–ï–ù–ù–´–ï –†–ê–ú–ö–ò –†–ï–ó–£–õ–¨–¢–ê–¢–ê:
    - –ë—ã—Å—Ç—Ä–æ (1-4 –Ω–µ–¥–µ–ª–∏)
    - –°—Ä–µ–¥–Ω–µ (1-3 –º–µ—Å—è—Ü–∞)
    - –î–æ–ª–≥–æ (3+ –º–µ—Å—è—Ü–µ–≤)
    
    –û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
    {{
        "title": "–ö—Ä–∞—Ç–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ (–¥–æ 110 —Å–∏–º–≤–æ–ª–æ–≤)",
        "summary": "–í —ç—Ç–æ–º –æ–±–∑–æ—Ä–µ —Ç—ã —É–∑–Ω–∞–µ—à—å –æ–± [–æ—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞], –∏ –≤–æ—Ç –Ω–∞—à–∞ –æ—Ü–µ–Ω–∫–∞ –¥–∞–Ω–Ω–æ–≥–æ –ø–æ—Å—Ç–∞: [–∫—Ä–∞—Ç–∫–∞—è –æ—Ü–µ–Ω–∫–∞]. –ò–¥–µ–∏ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ –∫–∞–∂–¥—ã–π –¥–µ–Ω—å: [2-3 –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∏–¥–µ–∏]",
        "overall_score": [–æ–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ 1-10],
        "scores": {{
            "practicality": [1-10],
            "novelty": [1-10],
            "depth": [1-10],
            "relevance": [1-10]
        }},
        "target_audience": ["–∞—É–¥–∏—Ç–æ—Ä–∏—è1", "–∞—É–¥–∏—Ç–æ—Ä–∏—è2"],
        "reading_time": "–≤—Ä–µ–º—è",
        "category": "–∫–∞—Ç–µ–≥–æ—Ä–∏—è",
        "complexity_level": "—É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏",
        "roi_potential": "ROI –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª",
        "business_stage": "—Å—Ç–∞–¥–∏—è –±–∏–∑–Ω–µ—Å–∞",
        "result_timeframe": "–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏",
        "action_checklist": ["–¥–µ–π—Å—Ç–≤–∏–µ 1", "–¥–µ–π—Å—Ç–≤–∏–µ 2", "–¥–µ–π—Å—Ç–≤–∏–µ 3"],
        "main_risks": ["—Ä–∏—Å–∫ 1", "—Ä–∏—Å–∫ 2"],
        "score_reason": "–û–±—ä—è—Å–Ω–µ–Ω–∏–µ –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–∏ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)",
        "content_quality": "{article.get('content_source', 'unknown')}"
    }}
    
    –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - –û—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
    - –§–æ—Ä–º–∞—Ç summary –¥–æ–ª–∂–µ–Ω —Å—Ç—Ä–æ–≥–æ —Å–ª–µ–¥–æ–≤–∞—Ç—å —à–∞–±–ª–æ–Ω—É "–í —ç—Ç–æ–º –æ–±–∑–æ—Ä–µ —Ç—ã —É–∑–Ω–∞–µ—à—å –æ–±..."
    - –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ = —Å—Ä–µ–¥–Ω–µ–µ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ 4 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    - action_checklist –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å 3-5 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
    - main_risks –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å 1-3 –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ä–∏—Å–∫–∞
    - –ë—É–¥—å –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã–º –≤ –æ—Ü–µ–Ω–∫–∞—Ö
    """
    
    # Prepare OpenAI API request
    url = "https://api.openai.com/v1/chat/completions"
    
    data = {
        "model": "gpt-4o-mini",
        "messages": [
            {
                "role": "system", 
                "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫ –±–∏–∑–Ω–µ—Å-–∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –º–Ω–æ–≥–æ–ª–µ—Ç–Ω–∏–º –æ–ø—ã—Ç–æ–º –≤ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–º –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏ –æ—Ü–µ–Ω–∫–µ —Ä–∏—Å–∫–æ–≤. –¢—ã —É–º–µ–µ—à—å –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª—ã –ø–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º, –æ–ø—Ä–µ–¥–µ–ª—è—Ç—å —Ü–µ–ª–µ–≤—É—é –∞—É–¥–∏—Ç–æ—Ä–∏—é –∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø–ª–∞–Ω—ã –¥–µ–π—Å—Ç–≤–∏–π. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ–±–∑–æ—Ä—ã —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏."
            },
            {
                "role": "user", 
                "content": prompt
            }
        ],
        "temperature": 0.2,
        "max_tokens": 1200,
        "response_format": {"type": "json_object"}
    }
    
    try:
        # Make the API request
        req = urllib.request.Request(url, 
                                   data=json.dumps(data).encode('utf-8'),
                                   headers={
                                       'Content-Type': 'application/json',
                                       'Authorization': f'Bearer {openai_api_key}'
                                   })
        
        with urllib.request.urlopen(req) as response:
            result = json.loads(response.read().decode('utf-8'))
            
            # Extract the response
            content = result['choices'][0]['message']['content']
            analysis_data = json.loads(content)
            
            # Validate and clean data
            validated_analysis = validate_enhanced_analysis(analysis_data, article)
            
            logger.info(f"‚úÖ Enhanced analysis generated! Overall Score: {validated_analysis['overall_score']}/10")
            return validated_analysis
            
    except Exception as e:
        logger.error(f"‚ùå Error generating enhanced analysis: {e}")
        return create_fallback_analysis(article)


def validate_enhanced_analysis(data, article):
    """Validate and clean enhanced analysis data."""
    # Validate overall score
    overall_score = data.get('overall_score', 7)
    if not isinstance(overall_score, (int, float)) or overall_score < 1 or overall_score > 10:
        overall_score = 7
    
    # Validate scores
    scores = data.get('scores', {})
    for key in ['practicality', 'novelty', 'depth', 'relevance']:
        if key not in scores or not isinstance(scores[key], (int, float)) or scores[key] < 1 or scores[key] > 10:
            scores[key] = 7
    
    # Recalculate overall score as average
    calculated_overall = round(sum(scores.values()) / len(scores))
    
    # Validate other fields
    validated_data = {
        'title': data.get('title', article['title'])[:110],
        'summary': data.get('summary', '–ö—Ä–∞—Ç–∫–∏–π –æ–±–∑–æ—Ä —Å—Ç–∞—Ç—å–∏'),
        'overall_score': calculated_overall,
        'scores': scores,
        'target_audience': data.get('target_audience', ['–Ω–∞—á–∏–Ω–∞—é—â–∏–µ –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–∏']),
        'reading_time': data.get('reading_time', '—Å—Ä–µ–¥–Ω–µ (15-30 –º–∏–Ω)'),
        'category': data.get('category', '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è'),
        'complexity_level': data.get('complexity_level', '—Å—Ä–µ–¥–Ω–∏–π'),
        'roi_potential': data.get('roi_potential', '—Å—Ä–µ–¥–Ω–∏–π'),
        'business_stage': data.get('business_stage', '—Ä–æ—Å—Ç (2-5 –ª–µ—Ç)'),
        'result_timeframe': data.get('result_timeframe', '—Å—Ä–µ–¥–Ω–µ (1-3 –º–µ—Å—è—Ü–∞)'),
        'action_checklist': data.get('action_checklist', ['–ò–∑—É—á–∏—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª', '–°–æ—Å—Ç–∞–≤–∏—Ç—å –ø–ª–∞–Ω', '–ù–∞—á–∞—Ç—å –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ']),
        'main_risks': data.get('main_risks', ['–ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ —Ä–µ—Å—É—Ä—Å–æ–≤', '–°–ª–æ–∂–Ω–æ—Å—Ç—å –≤–Ω–µ–¥—Ä–µ–Ω–∏—è']),
        'score_reason': data.get('score_reason', '–ü–æ–ª–µ–∑–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª —Å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏'),
        'content_quality': data.get('content_quality', article.get('content_source', 'unknown'))
    }
    
    return validated_data


def create_fallback_analysis(article):
    """Create fallback analysis when AI fails."""
    return {
        'title': article['title'][:110],
        'summary': '–í —ç—Ç–æ–º –æ–±–∑–æ—Ä–µ —Ç—ã —É–∑–Ω–∞–µ—à—å –æ–± –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö –±–∏–∑–Ω–µ—Å-–∏–¥–µ—è—Ö, –∏ –≤–æ—Ç –Ω–∞—à–∞ –æ—Ü–µ–Ω–∫–∞ –¥–∞–Ω–Ω–æ–≥–æ –ø–æ—Å—Ç–∞: –ø–æ–ª–µ–∑–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è. –ò–¥–µ–∏ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ –∫–∞–∂–¥—ã–π –¥–µ–Ω—å: –∏–∑—É—á–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã, –∞–¥–∞–ø—Ç–∏—Ä—É–π –ø–æ–¥ —Å–≤–æ—é —Å–∏—Ç—É–∞—Ü–∏—é, –Ω–∞—á–Ω–∏ —Å –º–∞–ª—ã—Ö —à–∞–≥–æ–≤.',
        'overall_score': 7,
        'scores': {
            'practicality': 7,
            'novelty': 6,
            'depth': 7,
            'relevance': 7
        },
        'target_audience': ['–Ω–∞—á–∏–Ω–∞—é—â–∏–µ –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–∏', '–æ–ø—ã—Ç–Ω—ã–µ –±–∏–∑–Ω–µ—Å–º–µ–Ω—ã'],
        'reading_time': '—Å—Ä–µ–¥–Ω–µ (15-30 –º–∏–Ω)',
        'category': '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è',
        'complexity_level': '—Å—Ä–µ–¥–Ω–∏–π',
        'roi_potential': '—Å—Ä–µ–¥–Ω–∏–π',
        'business_stage': '—Ä–æ—Å—Ç (2-5 –ª–µ—Ç)',
        'result_timeframe': '—Å—Ä–µ–¥–Ω–µ (1-3 –º–µ—Å—è—Ü–∞)',
        'action_checklist': ['–ò–∑—É—á–∏—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª –¥–µ—Ç–∞–ª—å–Ω–æ', '–í—ã–¥–µ–ª–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –∏–¥–µ–∏', '–°–æ—Å—Ç–∞–≤–∏—Ç—å –ø–ª–∞–Ω –≤–Ω–µ–¥—Ä–µ–Ω–∏—è', '–ù–∞—á–∞—Ç—å —Å –ø–∏–ª–æ—Ç–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞'],
        'main_risks': ['–ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –æ–ø—ã—Ç–∞', '–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã'],
        'score_reason': '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (–æ—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞)',
        'content_quality': article.get('content_source', 'fallback')
    }


def get_score_emoji(score):
    """Get emoji based on score."""
    if score >= 9:
        return "üî•"
    elif score >= 8:
        return "‚≠êÔ∏è"
    elif score >= 7:
        return "üëç"
    elif score >= 6:
        return "üëå"
    elif score >= 5:
        return "ü§î"
    else:
        return "üòê"


def get_category_emoji(category):
    """Get emoji for category."""
    category_emojis = {
        '—Å—Ç—Ä–∞—Ç–µ–≥–∏—è': 'üéØ',
        '–º–∞—Ä–∫–µ—Ç–∏–Ω–≥': 'üìà',
        '–ø—Ä–æ–¥–∞–∂–∏': 'üí∞',
        '—Ñ–∏–Ω–∞–Ω—Å—ã': 'üí≥',
        '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': 'üíª',
        '–ª–∏—á–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å': '‚ö°Ô∏è',
        '–∞–Ω–∞–ª–∏—Ç–∏–∫–∞': 'üìä'
    }
    return category_emojis.get(category.lower(), 'üìù')


def get_complexity_emoji(complexity):
    """Get emoji for complexity level."""
    if '–ø—Ä–æ—Å—Ç–æ–π' in complexity.lower():
        return 'üü¢'
    elif '—Å—Ä–µ–¥–Ω–∏–π' in complexity.lower():
        return 'üü°'
    else:
        return 'üî¥'


def get_roi_emoji(roi):
    """Get emoji for ROI potential."""
    if '–≤—ã—Å–æ–∫–∏–π' in roi.lower():
        return 'üíé'
    elif '—Å—Ä–µ–¥–Ω–∏–π' in roi.lower():
        return 'üí∞'
    else:
        return 'ü™ô'


def send_telegram_message(article, analysis):
    """Send enhanced message to Telegram."""
    logger.info("üì± Sending enhanced Telegram message...")
    
    telegram_token = os.environ.get('TELEGRAM_TOKEN')
    telegram_chat_id = os.environ.get('TELEGRAM_CHAT_ID')
    
    if not telegram_token or not telegram_chat_id:
        logger.error("‚ùå Telegram credentials not found")
        return False
    
    # Get current date
    current_date = datetime.now().strftime("%d.%m.%Y")
    
    # Get emojis
    score_emoji = get_score_emoji(analysis['overall_score'])
    category_emoji = get_category_emoji(analysis['category'])
    complexity_emoji = get_complexity_emoji(analysis['complexity_level'])
    roi_emoji = get_roi_emoji(analysis['roi_potential'])
    
    # Format target audience
    audience_text = ' ‚Ä¢ '.join(analysis['target_audience'])
    
    # Format action checklist
    action_list = '\n'.join([f"   ‚úÖ {action}" for action in analysis['action_checklist']])
    
    # Format risks
    risk_list = '\n'.join([f"   ‚ö†Ô∏è {risk}" for risk in analysis['main_risks']])
    
    # Content quality indicator
    content_indicator = ""
    if analysis.get('content_quality') == 'authenticated':
        content_indicator = "üîê –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (–∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è)"
    elif analysis.get('content_quality') == 'full_extraction':
        content_indicator = "üìÑ –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (–¥–æ paywall)"
    elif analysis.get('content_quality') == 'improved_extraction':
        content_indicator = "üìÑ –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (–¥–æ paywall)"
    elif analysis.get('content_quality') == 'fallback':
        content_indicator = "‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (–ø—É–±–ª–∏—á–Ω—ã–π –¥–æ—Å—Ç—É–ø)"
    else:
        content_indicator = "üìÑ RSS –∫–æ–Ω—Ç–µ–Ω—Ç"
    
    # Create the message
    message = f"""üåÖ <b>FastFounder Daily ‚Ä¢ {current_date}</b>

{score_emoji} <b>{analysis['overall_score']}/10</b> ‚Ä¢ {analysis['title']}

{analysis['summary']}

üìä <b>–ü—Ä–∞–∫—Ç–∏—á–Ω–æ—Å—Ç—å:</b> {analysis['scores']['practicality']}/10 | <b>–ù–æ–≤–∏–∑–Ω–∞:</b> {analysis['scores']['novelty']}/10 | <b>–ì–ª—É–±–∏–Ω–∞:</b> {analysis['scores']['depth']}/10 | <b>–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å:</b> {analysis['scores']['relevance']}/10

{category_emoji} <b>–ö–∞—Ç–µ–≥–æ—Ä–∏—è:</b> {analysis['category'].title()}
üë• <b>–î–ª—è –∫–æ–≥–æ:</b> {audience_text}
‚è± <b>–í—Ä–µ–º—è –∏–∑—É—á–µ–Ω–∏—è:</b> {analysis['reading_time']}

{complexity_emoji} <b>–°–ª–æ–∂–Ω–æ—Å—Ç—å:</b> {analysis['complexity_level']}
{roi_emoji} <b>ROI –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª:</b> {analysis['roi_potential']}
üè¢ <b>–°—Ç–∞–¥–∏—è –±–∏–∑–Ω–µ—Å–∞:</b> {analysis['business_stage']}
‚è∞ <b>–†–µ–∑—É–ª—å—Ç–∞—Ç —á–µ—Ä–µ–∑:</b> {analysis['result_timeframe']}

üìã <b>–ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π:</b>
{action_list}

‚ö†Ô∏è <b>–û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∏—Å–∫–∏:</b>
{risk_list}

üí° <b>AI –∞–Ω–∞–ª–∏–∑:</b> {analysis['score_reason']}

üîó {article['url']}

<i>{content_indicator}</i>
<i>ü§ñ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ FastFounder Bot</i>"""
    
    # Send message
    try:
        url = f"https://api.telegram.org/bot{telegram_token}/sendMessage"
        
        data = {
            'chat_id': telegram_chat_id,
            'text': message,
            'parse_mode': 'HTML',
            'disable_web_page_preview': False
        }
        
        encoded_data = urllib.parse.urlencode(data).encode('utf-8')
        
        req = urllib.request.Request(url, data=encoded_data)
        
        with urllib.request.urlopen(req) as response:
            result = json.loads(response.read().decode('utf-8'))
            
            if result.get('ok'):
                logger.info("‚úÖ Telegram message sent successfully!")
                return True
            else:
                logger.error(f"‚ùå Telegram API error: {result}")
                return False
                
    except Exception as e:
        logger.error(f"‚ùå Error sending Telegram message: {e}")
        return False


def main():
    """Main function."""
    logger.info("üöÄ Starting FastFounder Daily Bot (Authenticated Version)")
    
    # Initialize authenticated scraper
    scraper = None
    email = os.environ.get('FAST_FOUNDER_EMAIL')
    password = os.environ.get('FAST_FOUNDER_PASSWORD')
    
    if email and password:
        logger.info("üîê Credentials found, initializing authenticated scraper...")
        scraper = FastFounderAuthenticatedScraper()
        if scraper.login(email, password):
            logger.info("‚úÖ Authentication successful - will use full content")
        else:
            logger.warning("‚ö†Ô∏è Authentication failed - will use fallback scraping")
            scraper = None
    else:
        logger.warning("‚ö†Ô∏è No FastFounder credentials found - using fallback scraping")
    
    # Get RSS feed
    articles = get_rss_feed()
    
    if not articles:
        logger.error("‚ùå No articles found in RSS feed")
        return
    
    # Process the latest article
    article = articles[0]
    logger.info(f"üì∞ Processing article: {article['title']}")
    
    # Scrape article content (authenticated or fallback)
    article_data = scrape_article_content_authenticated(article, scraper)
    
    # Generate enhanced analysis
    analysis = generate_enhanced_analysis(article_data)
    
    # Send to Telegram
    success = send_telegram_message(article_data, analysis)
    
    if success:
        logger.info("üéâ Daily digest sent successfully!")
    else:
        logger.error("‚ùå Failed to send daily digest")


if __name__ == "__main__":
    main() 